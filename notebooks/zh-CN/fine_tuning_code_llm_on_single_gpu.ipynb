{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNdZ-kD0l78P"
      },
      "source": [
        "#  在单个 GPU 上针对自定义代码微调代码 LLM\n",
        "\n",
        "_作者: [Maria Khalusova](https://github.com/MKhalusova)_\n",
        "\n",
        "公开发布的代码 LLM，如 Codex、StarCoder 和 Code Llama，在生成遵循通用编程原则和语法的代码方面表现出色，但它们可能不符合组织的内部惯例，或者不了解某些特定的库。\n",
        "\n",
        "在这个 notebook 中，我们将展示如何微调代码 LLM 来更好的理解你们公司或组织的代码风格和习惯。由于代码 LLM 非常大，按照传统的微调方式可能会消耗大量资源。但不用担心！我们会教你一些技巧，让你只用单个 GPU 就能完成微调工作。\n",
        "\n",
        "\n",
        "## 数据集\n",
        "\n",
        "对于这个例子，我们选择了 GitHub 上 Hugging Face 的前 10 个公共仓库。我们已经排除了非代码文件，如图片、音频文件、演示文稿等。对于 Jupyter notebook，我们只保留了包含代码的单元格。生成的代码被存储为一个数据集，你可以在 Hugging Face Hub 上找到，位于 [`smangrul/hf-stack-v1`](https://huggingface.co/datasets/smangrul/hf-stack-v1)。它包含仓库 id、文件路径和文件内容。\n",
        "\n",
        "\n",
        "## 模型\n",
        "\n",
        "我们将微调 [`bigcode/starcoderbase-1b`](https://huggingface.co/bigcode/starcoderbase-1b) 模型，这是一个在 80 多种编程语言上训练的 10 亿参数模型。这是一个需要权限的模型，所以如果你计划使用这个确切模型运行这个 notebook，你需要在其模型页面上获得访问权限。登录你的 Hugging Face 帐户以执行此操作：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bPlCJYDK6vrF",
        "outputId": "a4b65a80-a542-4168-f023-0b95a38e199f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "499f308a33e342b3869f35cf9ac5eb67",
            "4cef64cd3d5b4fe699008fc40ec612c3",
            "a1c6a15eaec34748ab1e75ec1add3b01",
            "dc87041381b94558939f626b2a93e94f",
            "b167e021a19e420fa91f876f81a7ab12",
            "ca8e7935581a4d67ad0e9b7f4f2ed128",
            "939c40c9931646a8b609d81b678511d1",
            "6cc426f9bf61475e99f8056a482a4c80",
            "46198864372549df8a442e81257e3837",
            "3f52234077e242a19600f823412232fb",
            "320b27911e994b6590f5dd8e040463cc",
            "16a663c77fd74a6e8f89af60ade0c8d4",
            "2e20951121bd42e0b6589a3bafc48ef5",
            "c212a6553c4a40908417006181452847",
            "6c09de25b5f84813a189bee2ef2e1a36",
            "b390ada147664c8192dc6ff802961129",
            "758dd23fb8fa4dfa9aaf5ad754dc6ec4",
            "f9e97434bb8541fa9a85b6248b3350c5",
            "d003c198c8794e7f9138f4382189c62b",
            "7aecccd20ea24efdb3778e9063b9cfe6"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "499f308a33e342b3869f35cf9ac5eb67"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMVe_c8q43Qo"
      },
      "source": [
        "\n",
        "\n",
        "To get started, let's install all the necessary libraries. As you can see, in addition to `transformers` and `datasets`, we'll be using `peft`, `bitsandbytes`, and `flash-attn` to optimize the training.\n",
        "\n",
        "By employing parameter-efficient training techniques, we can run this notebook on a single A100 High-RAM GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Fp7i8WMCjKJG",
        "outputId": "ad090c1e-c26e-4b12-b41c-637d1e91f2d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/3.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets peft bitsandbytes flash-attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16EdABzt3_Ig"
      },
      "source": [
        "现在让我们定义一些变量。请随意调整这些变量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hru3G-CLmqis"
      },
      "outputs": [],
      "source": [
        "MODEL=\"meta-llama/Llama-3.2-3B\" # Model checkpoint on the Hugging Face Hub\n",
        "DATASET=\"smangrul/hf-stack-v1\"   # Dataset on the Hugging Face Hub\n",
        "DATA_COLUMN=\"content\"            # Column name containing the code content\n",
        "\n",
        "SEQ_LENGTH=2048                  # Sequence length\n",
        "\n",
        "# Training arguments\n",
        "MAX_STEPS=2000                   # max_steps\n",
        "BATCH_SIZE=16                    # batch_size\n",
        "GR_ACC_STEPS=1                   # gradient_accumulation_steps\n",
        "LR=5e-4                          # learning_rate\n",
        "LR_SCHEDULER_TYPE=\"cosine\"       # lr_scheduler_type\n",
        "WEIGHT_DECAY=0.01                # weight_decay\n",
        "NUM_WARMUP_STEPS=30              # num_warmup_steps\n",
        "EVAL_FREQ=100                    # eval_freq\n",
        "SAVE_FREQ=100                    # save_freq\n",
        "LOG_FREQ=25                      # log_freq\n",
        "OUTPUT_DIR=\"peft-starcoder-lora-a100\" # output_dir\n",
        "BF16=True                        # bf16\n",
        "FP16=False                       # no_fp16\n",
        "\n",
        "# FIM trasformations arguments\n",
        "FIM_RATE=0.5                     # fim_rate\n",
        "FIM_SPM_RATE=0.5                 # fim_spm_rate\n",
        "\n",
        "# LORA\n",
        "LORA_R=8                         # lora_r\n",
        "LORA_ALPHA=32                    # lora_alpha\n",
        "LORA_DROPOUT=0.0                 # lora_dropout\n",
        "LORA_TARGET_MODULES=\"c_proj,c_attn,q_attn,c_fc,c_proj\"    # lora_target_modules\n",
        "\n",
        "# bitsandbytes config\n",
        "USE_NESTED_QUANT=True            # use_nested_quant\n",
        "BNB_4BIT_COMPUTE_DTYPE=\"bfloat16\"# bnb_4bit_compute_dtype\n",
        "\n",
        "SEED=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FyZSXTbJrcnC"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    set_seed,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "reLtOfml2JH-",
        "outputId": "b439d9da-3790-4ae0-b3bf-5aec9cc0cefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                            Version\n",
            "---------------------------------- -------------------\n",
            "absl-py                            1.4.0\n",
            "accelerate                         1.1.1\n",
            "aiohappyeyeballs                   2.4.4\n",
            "aiohttp                            3.11.9\n",
            "aiosignal                          1.3.1\n",
            "alabaster                          1.0.0\n",
            "albucore                           0.0.19\n",
            "albumentations                     1.4.20\n",
            "altair                             4.2.2\n",
            "annotated-types                    0.7.0\n",
            "anyio                              3.7.1\n",
            "argon2-cffi                        23.1.0\n",
            "argon2-cffi-bindings               21.2.0\n",
            "array_record                       0.5.1\n",
            "arviz                              0.20.0\n",
            "astropy                            6.1.7\n",
            "astropy-iers-data                  0.2024.12.2.0.35.34\n",
            "astunparse                         1.6.3\n",
            "async-timeout                      4.0.3\n",
            "atpublic                           4.1.0\n",
            "attrs                              24.2.0\n",
            "audioread                          3.0.1\n",
            "autograd                           1.7.0\n",
            "babel                              2.16.0\n",
            "backcall                           0.2.0\n",
            "beautifulsoup4                     4.12.3\n",
            "bigframes                          1.27.0\n",
            "bigquery-magics                    0.4.0\n",
            "bleach                             6.2.0\n",
            "blinker                            1.9.0\n",
            "blis                               0.7.11\n",
            "blosc2                             2.7.1\n",
            "bokeh                              3.6.2\n",
            "Bottleneck                         1.4.2\n",
            "bqplot                             0.12.43\n",
            "branca                             0.8.0\n",
            "CacheControl                       0.14.1\n",
            "cachetools                         5.5.0\n",
            "catalogue                          2.0.10\n",
            "certifi                            2024.8.30\n",
            "cffi                               1.17.1\n",
            "chardet                            5.2.0\n",
            "charset-normalizer                 3.4.0\n",
            "chex                               0.1.87\n",
            "clarabel                           0.9.0\n",
            "click                              8.1.7\n",
            "cloudpathlib                       0.20.0\n",
            "cloudpickle                        3.1.0\n",
            "cmake                              3.30.5\n",
            "cmdstanpy                          1.2.4\n",
            "colorcet                           3.1.0\n",
            "colorlover                         0.3.0\n",
            "colour                             0.1.5\n",
            "community                          1.0.0b1\n",
            "confection                         0.1.5\n",
            "cons                               0.4.6\n",
            "contourpy                          1.3.1\n",
            "cryptography                       43.0.3\n",
            "cuda-python                        12.2.1\n",
            "cudf-cu12                          24.10.1\n",
            "cufflinks                          0.17.3\n",
            "cupy-cuda12x                       12.2.0\n",
            "cvxopt                             1.3.2\n",
            "cvxpy                              1.5.4\n",
            "cycler                             0.12.1\n",
            "cymem                              2.0.10\n",
            "Cython                             3.0.11\n",
            "dask                               2024.10.0\n",
            "datascience                        0.17.6\n",
            "datasets                           3.1.0\n",
            "db-dtypes                          1.3.1\n",
            "dbus-python                        1.2.18\n",
            "debugpy                            1.8.0\n",
            "decorator                          4.4.2\n",
            "defusedxml                         0.7.1\n",
            "Deprecated                         1.2.15\n",
            "diffusers                          0.31.0\n",
            "dill                               0.3.8\n",
            "distro                             1.9.0\n",
            "dlib                               19.24.2\n",
            "dm-tree                            0.1.8\n",
            "docker-pycreds                     0.4.0\n",
            "docstring_parser                   0.16\n",
            "docutils                           0.21.2\n",
            "dopamine_rl                        4.0.9\n",
            "duckdb                             1.1.3\n",
            "earthengine-api                    1.2.0\n",
            "easydict                           1.13\n",
            "ecos                               2.0.14\n",
            "editdistance                       0.8.1\n",
            "eerepr                             0.0.4\n",
            "einops                             0.8.0\n",
            "en-core-web-sm                     3.7.1\n",
            "entrypoints                        0.4\n",
            "et_xmlfile                         2.0.0\n",
            "etils                              1.11.0\n",
            "etuples                            0.3.9\n",
            "eval_type_backport                 0.2.0\n",
            "exceptiongroup                     1.2.2\n",
            "fastai                             2.7.18\n",
            "fastcore                           1.7.22\n",
            "fastdownload                       0.0.7\n",
            "fastjsonschema                     2.21.1\n",
            "fastprogress                       1.0.3\n",
            "fastrlock                          0.8.2\n",
            "filelock                           3.16.1\n",
            "firebase-admin                     6.5.0\n",
            "Flask                              3.0.3\n",
            "flatbuffers                        24.3.25\n",
            "flax                               0.8.5\n",
            "folium                             0.18.0\n",
            "fonttools                          4.55.1\n",
            "frozendict                         2.4.6\n",
            "frozenlist                         1.5.0\n",
            "fsspec                             2024.9.0\n",
            "future                             1.0.0\n",
            "gast                               0.6.0\n",
            "gcsfs                              2024.10.0\n",
            "GDAL                               3.6.4\n",
            "gdown                              5.2.0\n",
            "geemap                             0.35.1\n",
            "gensim                             4.3.3\n",
            "geocoder                           1.38.1\n",
            "geographiclib                      2.0\n",
            "geopandas                          1.0.1\n",
            "geopy                              2.4.1\n",
            "gin-config                         0.5.0\n",
            "gitdb                              4.0.11\n",
            "GitPython                          3.1.43\n",
            "glob2                              0.7\n",
            "google                             2.0.3\n",
            "google-ai-generativelanguage       0.6.10\n",
            "google-api-core                    2.19.2\n",
            "google-api-python-client           2.151.0\n",
            "google-auth                        2.27.0\n",
            "google-auth-httplib2               0.2.0\n",
            "google-auth-oauthlib               1.2.1\n",
            "google-cloud-aiplatform            1.73.0\n",
            "google-cloud-bigquery              3.25.0\n",
            "google-cloud-bigquery-connection   1.16.1\n",
            "google-cloud-bigquery-storage      2.27.0\n",
            "google-cloud-bigtable              2.27.0\n",
            "google-cloud-core                  2.4.1\n",
            "google-cloud-datastore             2.20.1\n",
            "google-cloud-firestore             2.19.0\n",
            "google-cloud-functions             1.18.1\n",
            "google-cloud-iam                   2.16.1\n",
            "google-cloud-language              2.15.1\n",
            "google-cloud-pubsub                2.27.1\n",
            "google-cloud-resource-manager      1.13.1\n",
            "google-cloud-storage               2.8.0\n",
            "google-cloud-translate             3.17.0\n",
            "google-colab                       1.0.0\n",
            "google-crc32c                      1.6.0\n",
            "google-generativeai                0.8.3\n",
            "google-pasta                       0.2.0\n",
            "google-resumable-media             2.7.2\n",
            "googleapis-common-protos           1.66.0\n",
            "googledrivedownloader              0.4\n",
            "graphviz                           0.20.3\n",
            "greenlet                           3.1.1\n",
            "grpc-google-iam-v1                 0.13.1\n",
            "grpcio                             1.68.1\n",
            "grpcio-status                      1.62.3\n",
            "gspread                            6.0.2\n",
            "gspread-dataframe                  3.3.1\n",
            "gym                                0.25.2\n",
            "gym-notices                        0.0.8\n",
            "h11                                0.14.0\n",
            "h5netcdf                           1.4.1\n",
            "h5py                               3.12.1\n",
            "holidays                           0.62\n",
            "holoviews                          1.20.0\n",
            "html5lib                           1.1\n",
            "httpcore                           1.0.7\n",
            "httpimport                         1.4.0\n",
            "httplib2                           0.22.0\n",
            "httpx                              0.28.0\n",
            "huggingface-hub                    0.26.3\n",
            "humanize                           4.11.0\n",
            "hyperopt                           0.2.7\n",
            "ibis-framework                     9.2.0\n",
            "idna                               3.10\n",
            "imageio                            2.36.1\n",
            "imageio-ffmpeg                     0.5.1\n",
            "imagesize                          1.4.1\n",
            "imbalanced-learn                   0.12.4\n",
            "imgaug                             0.4.0\n",
            "immutabledict                      4.2.1\n",
            "importlib_metadata                 8.5.0\n",
            "importlib_resources                6.4.5\n",
            "imutils                            0.5.4\n",
            "inflect                            7.4.0\n",
            "iniconfig                          2.0.0\n",
            "intel-cmplr-lib-ur                 2025.0.3\n",
            "intel-openmp                       2025.0.3\n",
            "ipyevents                          2.0.2\n",
            "ipyfilechooser                     0.6.0\n",
            "ipykernel                          5.5.6\n",
            "ipyleaflet                         0.19.2\n",
            "ipyparallel                        8.8.0\n",
            "ipython                            7.34.0\n",
            "ipython-genutils                   0.2.0\n",
            "ipython-sql                        0.5.0\n",
            "ipytree                            0.2.2\n",
            "ipywidgets                         7.7.1\n",
            "itsdangerous                       2.2.0\n",
            "jax                                0.4.33\n",
            "jax-cuda12-pjrt                    0.4.33\n",
            "jax-cuda12-plugin                  0.4.33\n",
            "jaxlib                             0.4.33\n",
            "jeepney                            0.7.1\n",
            "jellyfish                          1.1.2\n",
            "jieba                              0.42.1\n",
            "Jinja2                             3.1.4\n",
            "jiter                              0.8.0\n",
            "joblib                             1.4.2\n",
            "jsonpatch                          1.33\n",
            "jsonpickle                         4.0.0\n",
            "jsonpointer                        3.0.0\n",
            "jsonschema                         4.23.0\n",
            "jsonschema-specifications          2024.10.1\n",
            "jupyter-client                     6.1.12\n",
            "jupyter-console                    6.1.0\n",
            "jupyter_core                       5.7.2\n",
            "jupyter-leaflet                    0.19.2\n",
            "jupyter-server                     1.24.0\n",
            "jupyterlab_pygments                0.3.0\n",
            "jupyterlab_widgets                 3.0.13\n",
            "kaggle                             1.6.17\n",
            "kagglehub                          0.3.4\n",
            "keras                              3.5.0\n",
            "keyring                            23.5.0\n",
            "kiwisolver                         1.4.7\n",
            "langchain                          0.3.9\n",
            "langchain-core                     0.3.21\n",
            "langchain-text-splitters           0.3.2\n",
            "langcodes                          3.5.0\n",
            "langsmith                          0.1.147\n",
            "language_data                      1.3.0\n",
            "launchpadlib                       1.10.16\n",
            "lazr.restfulclient                 0.14.4\n",
            "lazr.uri                           1.0.6\n",
            "lazy_loader                        0.4\n",
            "libclang                           18.1.1\n",
            "libcudf-cu12                       24.10.1\n",
            "librosa                            0.10.2.post1\n",
            "lightgbm                           4.5.0\n",
            "linkify-it-py                      2.0.3\n",
            "llvmlite                           0.43.0\n",
            "locket                             1.0.0\n",
            "logical-unification                0.4.6\n",
            "lxml                               5.3.0\n",
            "marisa-trie                        1.2.1\n",
            "Markdown                           3.7\n",
            "markdown-it-py                     3.0.0\n",
            "MarkupSafe                         3.0.2\n",
            "matplotlib                         3.8.0\n",
            "matplotlib-inline                  0.1.7\n",
            "matplotlib-venn                    1.1.1\n",
            "mdit-py-plugins                    0.4.2\n",
            "mdurl                              0.1.2\n",
            "miniKanren                         1.0.3\n",
            "missingno                          0.5.2\n",
            "mistune                            3.0.2\n",
            "mizani                             0.13.0\n",
            "mkl                                2025.0.1\n",
            "ml-dtypes                          0.4.1\n",
            "mlxtend                            0.23.3\n",
            "more-itertools                     10.5.0\n",
            "moviepy                            1.0.3\n",
            "mpmath                             1.3.0\n",
            "msgpack                            1.1.0\n",
            "multidict                          6.1.0\n",
            "multipledispatch                   1.0.0\n",
            "multiprocess                       0.70.16\n",
            "multitasking                       0.0.11\n",
            "murmurhash                         1.0.11\n",
            "music21                            9.3.0\n",
            "namex                              0.0.8\n",
            "natsort                            8.4.0\n",
            "nbclassic                          1.1.0\n",
            "nbclient                           0.10.1\n",
            "nbconvert                          7.16.4\n",
            "nbformat                           5.10.4\n",
            "ndindex                            1.9.2\n",
            "nest-asyncio                       1.6.0\n",
            "networkx                           3.4.2\n",
            "nibabel                            5.3.2\n",
            "nltk                               3.9.1\n",
            "notebook                           6.5.5\n",
            "notebook_shim                      0.2.4\n",
            "numba                              0.60.0\n",
            "numexpr                            2.10.2\n",
            "numpy                              1.26.4\n",
            "nvidia-cublas-cu12                 12.6.4.1\n",
            "nvidia-cuda-cupti-cu12             12.6.80\n",
            "nvidia-cuda-nvcc-cu12              12.6.85\n",
            "nvidia-cuda-runtime-cu12           12.6.77\n",
            "nvidia-cudnn-cu12                  9.6.0.74\n",
            "nvidia-cufft-cu12                  11.3.0.4\n",
            "nvidia-curand-cu12                 10.3.7.77\n",
            "nvidia-cusolver-cu12               11.7.1.2\n",
            "nvidia-cusparse-cu12               12.5.4.2\n",
            "nvidia-nccl-cu12                   2.23.4\n",
            "nvidia-nvjitlink-cu12              12.6.85\n",
            "nvtx                               0.2.10\n",
            "nx-cugraph-cu12                    24.10.0\n",
            "oauth2client                       4.1.3\n",
            "oauthlib                           3.2.2\n",
            "openai                             1.54.5\n",
            "opencv-contrib-python              4.10.0.84\n",
            "opencv-python                      4.10.0.84\n",
            "opencv-python-headless             4.10.0.84\n",
            "openpyxl                           3.1.5\n",
            "opentelemetry-api                  1.28.2\n",
            "opentelemetry-sdk                  1.28.2\n",
            "opentelemetry-semantic-conventions 0.49b2\n",
            "opt_einsum                         3.4.0\n",
            "optax                              0.2.4\n",
            "optree                             0.13.1\n",
            "orbax-checkpoint                   0.6.4\n",
            "orjson                             3.10.12\n",
            "osqp                               0.6.7.post3\n",
            "packaging                          24.2\n",
            "pandas                             2.2.2\n",
            "pandas-datareader                  0.10.0\n",
            "pandas-gbq                         0.24.0\n",
            "pandas-stubs                       2.2.2.240909\n",
            "pandocfilters                      1.5.1\n",
            "panel                              1.5.4\n",
            "param                              2.1.1\n",
            "parso                              0.8.4\n",
            "parsy                              2.1\n",
            "partd                              1.4.2\n",
            "pathlib                            1.0.1\n",
            "patsy                              1.0.1\n",
            "peewee                             3.17.8\n",
            "peft                               0.13.2\n",
            "pexpect                            4.9.0\n",
            "pickleshare                        0.7.5\n",
            "pillow                             11.0.0\n",
            "pip                                24.1.2\n",
            "platformdirs                       4.3.6\n",
            "plotly                             5.24.1\n",
            "plotnine                           0.14.3\n",
            "pluggy                             1.5.0\n",
            "ply                                3.11\n",
            "polars                             1.9.0\n",
            "pooch                              1.8.2\n",
            "portpicker                         1.5.2\n",
            "preshed                            3.0.9\n",
            "prettytable                        3.12.0\n",
            "proglog                            0.1.10\n",
            "progressbar2                       4.5.0\n",
            "prometheus_client                  0.21.1\n",
            "promise                            2.3\n",
            "prompt_toolkit                     3.0.48\n",
            "propcache                          0.2.1\n",
            "prophet                            1.1.6\n",
            "proto-plus                         1.25.0\n",
            "protobuf                           4.25.5\n",
            "psutil                             5.9.5\n",
            "psycopg2                           2.9.10\n",
            "ptyprocess                         0.7.0\n",
            "py-cpuinfo                         9.0.0\n",
            "py4j                               0.10.9.7\n",
            "pyarrow                            17.0.0\n",
            "pyarrow-hotfix                     0.6\n",
            "pyasn1                             0.6.1\n",
            "pyasn1_modules                     0.4.1\n",
            "pycocotools                        2.0.8\n",
            "pycparser                          2.22\n",
            "pydantic                           2.10.3\n",
            "pydantic_core                      2.27.1\n",
            "pydata-google-auth                 1.9.0\n",
            "pydot                              3.0.3\n",
            "pydotplus                          2.0.2\n",
            "PyDrive                            1.3.1\n",
            "PyDrive2                           1.21.3\n",
            "pyerfa                             2.0.1.5\n",
            "pygame                             2.6.1\n",
            "pygit2                             1.16.0\n",
            "Pygments                           2.18.0\n",
            "PyGObject                          3.42.1\n",
            "PyJWT                              2.10.1\n",
            "pylibcudf-cu12                     24.10.1\n",
            "pylibcugraph-cu12                  24.10.0\n",
            "pylibraft-cu12                     24.10.0\n",
            "pymc                               5.18.2\n",
            "pymystem3                          0.2.0\n",
            "pynvjitlink-cu12                   0.4.0\n",
            "pyogrio                            0.10.0\n",
            "Pyomo                              6.8.2\n",
            "PyOpenGL                           3.1.7\n",
            "pyOpenSSL                          24.2.1\n",
            "pyparsing                          3.2.0\n",
            "pyperclip                          1.9.0\n",
            "pyproj                             3.7.0\n",
            "pyshp                              2.3.1\n",
            "PySocks                            1.7.1\n",
            "pyspark                            3.5.3\n",
            "pytensor                           2.26.4\n",
            "pytest                             8.3.4\n",
            "python-apt                         0.0.0\n",
            "python-box                         7.2.0\n",
            "python-dateutil                    2.8.2\n",
            "python-louvain                     0.16\n",
            "python-slugify                     8.0.4\n",
            "python-utils                       3.9.1\n",
            "pytz                               2024.2\n",
            "pyviz_comms                        3.0.3\n",
            "PyYAML                             6.0.2\n",
            "pyzmq                              24.0.1\n",
            "qdldl                              0.1.7.post4\n",
            "ratelim                            0.1.6\n",
            "referencing                        0.35.1\n",
            "regex                              2024.9.11\n",
            "requests                           2.32.3\n",
            "requests-oauthlib                  1.3.1\n",
            "requests-toolbelt                  1.0.0\n",
            "requirements-parser                0.9.0\n",
            "rich                               13.9.4\n",
            "rmm-cu12                           24.10.0\n",
            "rpds-py                            0.22.3\n",
            "rpy2                               3.4.2\n",
            "rsa                                4.9\n",
            "safetensors                        0.4.5\n",
            "scikit-image                       0.24.0\n",
            "scikit-learn                       1.5.2\n",
            "scipy                              1.13.1\n",
            "scooby                             0.10.0\n",
            "scs                                3.2.7\n",
            "seaborn                            0.13.2\n",
            "SecretStorage                      3.3.1\n",
            "Send2Trash                         1.8.3\n",
            "sentence-transformers              3.2.1\n",
            "sentencepiece                      0.2.0\n",
            "sentry-sdk                         2.19.0\n",
            "setproctitle                       1.3.4\n",
            "setuptools                         75.1.0\n",
            "shap                               0.46.0\n",
            "shapely                            2.0.6\n",
            "shellingham                        1.5.4\n",
            "simple-parsing                     0.1.6\n",
            "six                                1.16.0\n",
            "sklearn-pandas                     2.2.0\n",
            "slicer                             0.0.8\n",
            "smart-open                         7.0.5\n",
            "smmap                              5.0.1\n",
            "sniffio                            1.3.1\n",
            "snowballstemmer                    2.2.0\n",
            "soundfile                          0.12.1\n",
            "soupsieve                          2.6\n",
            "soxr                               0.5.0.post1\n",
            "spacy                              3.7.5\n",
            "spacy-legacy                       3.0.12\n",
            "spacy-loggers                      1.0.5\n",
            "Sphinx                             8.1.3\n",
            "sphinxcontrib-applehelp            2.0.0\n",
            "sphinxcontrib-devhelp              2.0.0\n",
            "sphinxcontrib-htmlhelp             2.1.0\n",
            "sphinxcontrib-jsmath               1.0.1\n",
            "sphinxcontrib-qthelp               2.0.0\n",
            "sphinxcontrib-serializinghtml      2.0.0\n",
            "SQLAlchemy                         2.0.36\n",
            "sqlglot                            25.1.0\n",
            "sqlparse                           0.5.2\n",
            "srsly                              2.4.8\n",
            "stanio                             0.5.1\n",
            "statsmodels                        0.14.4\n",
            "StrEnum                            0.4.15\n",
            "stringzilla                        3.11.0\n",
            "sympy                              1.13.1\n",
            "tables                             3.10.1\n",
            "tabulate                           0.9.0\n",
            "tbb                                2022.0.0\n",
            "tcmlib                             1.2.0\n",
            "tenacity                           9.0.0\n",
            "tensorboard                        2.17.1\n",
            "tensorboard-data-server            0.7.2\n",
            "tensorflow                         2.17.1\n",
            "tensorflow-datasets                4.9.7\n",
            "tensorflow-hub                     0.16.1\n",
            "tensorflow-io-gcs-filesystem       0.37.1\n",
            "tensorflow-metadata                1.13.1\n",
            "tensorflow-probability             0.24.0\n",
            "tensorstore                        0.1.69\n",
            "termcolor                          2.5.0\n",
            "terminado                          0.18.1\n",
            "text-unidecode                     1.3\n",
            "textblob                           0.17.1\n",
            "tf_keras                           2.17.0\n",
            "tf-slim                            1.1.0\n",
            "thinc                              8.2.5\n",
            "threadpoolctl                      3.5.0\n",
            "tifffile                           2024.9.20\n",
            "timm                               1.0.12\n",
            "tinycss2                           1.4.0\n",
            "tokenizers                         0.20.3\n",
            "toml                               0.10.2\n",
            "tomli                              2.2.1\n",
            "toolz                              0.12.1\n",
            "torch                              2.5.1+cu121\n",
            "torchaudio                         2.5.1+cu121\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.20.1+cu121\n",
            "tornado                            6.3.3\n",
            "tqdm                               4.66.6\n",
            "traitlets                          5.7.1\n",
            "traittypes                         0.2.1\n",
            "transformers                       4.46.3\n",
            "tweepy                             4.14.0\n",
            "typeguard                          4.4.1\n",
            "typer                              0.15.0\n",
            "types-pytz                         2024.2.0.20241003\n",
            "types-setuptools                   75.6.0.20241126\n",
            "typing_extensions                  4.12.2\n",
            "tzdata                             2024.2\n",
            "tzlocal                            5.2\n",
            "uc-micro-py                        1.0.3\n",
            "umf                                0.9.1\n",
            "uritemplate                        4.1.1\n",
            "urllib3                            2.2.3\n",
            "vega-datasets                      0.9.0\n",
            "wadllib                            1.3.6\n",
            "wandb                              0.18.7\n",
            "wasabi                             1.1.3\n",
            "wcwidth                            0.2.13\n",
            "weasel                             0.4.1\n",
            "webcolors                          24.11.1\n",
            "webencodings                       0.5.1\n",
            "websocket-client                   1.8.0\n",
            "Werkzeug                           3.1.3\n",
            "wheel                              0.45.1\n",
            "widgetsnbextension                 3.6.10\n",
            "wordcloud                          1.9.4\n",
            "wrapt                              1.17.0\n",
            "xarray                             2024.10.0\n",
            "xarray-einstats                    0.8.0\n",
            "xgboost                            2.1.3\n",
            "xlrd                               2.0.1\n",
            "xxhash                             3.5.0\n",
            "xyzservices                        2024.9.0\n",
            "yarl                               1.18.3\n",
            "yellowbrick                        1.5\n",
            "yfinance                           0.2.50\n",
            "zipp                               3.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO7F5L5AtKo1"
      },
      "source": [
        "## 准备数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LmrIZqP0oUE"
      },
      "source": [
        "首先加载数据。由于数据集可能相当大，请确保启用流模式。流模式允许我们在遍历数据集时逐步加载数据，而不是一次性下载数据集的整个内容。\n",
        "\n",
        "我们将前 4000 个示例作为验证集，其余的全部作为训练数据。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4oJZvZb-1J88"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "dataset = load_dataset(\n",
        "    DATASET,\n",
        "    data_dir=\"data\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        ")\n",
        "\n",
        "valid_data = dataset.take(4000)\n",
        "train_data = dataset.skip(4000)\n",
        "train_data = train_data.shuffle(buffer_size=5000, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLQ8t0LM2GR6"
      },
      "source": [
        "在这一步，数据集仍然包含任意长度的原始数据。为了训练，我们需要固定长度的输入。让我们创建一个可迭代的数据集，它可以从文本文件流中返回固定长度的 token 块。\n",
        "\n",
        "首先，让我们估计数据集中每个 token 的平均字符数，这将帮助我们稍后估计文本缓冲区中的 token 数量。默认情况下，我们只从数据集中取 400 个示例（`nb_examples`）。只使用整个数据集的一个子集可以减少计算成本，同时仍然提供了对整体字符到 token 比的合理估计。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCiAvydztNsu",
        "outputId": "41109ff5-d5f6-4eba-9fb1-51f284f4dacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [00:05<00:00, 75.62it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The character to token ratio of the dataset is: 3.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
        "\n",
        "def chars_token_ratio(dataset, tokenizer, data_column, nb_examples=400):\n",
        "    \"\"\"\n",
        "    Estimate the average number of characters per token in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    total_characters, total_tokens = 0, 0\n",
        "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
        "        total_characters += len(example[data_column])\n",
        "        total_tokens += len(tokenizer(example[data_column]).tokens())\n",
        "\n",
        "    return total_characters / total_tokens\n",
        "\n",
        "\n",
        "chars_per_token = chars_token_ratio(train_data, tokenizer, DATA_COLUMN)\n",
        "print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F13VGobB3Ma"
      },
      "source": [
        "字符到 token 的比也可以用作文本标记质量的一个指标。例如，字符到 token 的比为 1.0 意味着每个字符都由一个 token 表示，这并没有太多意义。表明标记化做得不好。在标准的英文文本中，一个 token 通常相当于大约四个字符，这意味着字符到 token 的比率大约是 4.0。我们可以预见在代码数据集中的比率会更低，但一般来说，2.0 到 3.5 之间的数字可以认为是足够好的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcwYFRPpwxea"
      },
      "source": [
        "**可选的 FIM 变换**\n",
        "自回归语言模型通常是从左到右生成序列的。通过应用 FIM 变换，模型也可以学习填充文本。详细信息可以看[\"Efficient Training of Language Models to Fill in the Middle\" 这篇论文](https://arxiv.org/pdf/2207.14255.pdf)了解这种技术。\n",
        "\n",
        "我们将在下面定义 FIM 变换，并在创建可迭代数据集时使用它们。然而，如果你想省略变换步骤，请将 `fim_rate` 设置为 0。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zmejYvEKw1E-"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Helper function to get token ids of the special tokens for prefix, suffix and middle for FIM transformations.\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def get_fim_token_ids(tokenizer):\n",
        "    try:\n",
        "        FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD = tokenizer.special_tokens_map[\"additional_special_tokens\"][1:5]\n",
        "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = (\n",
        "            tokenizer.vocab[tok] for tok in [FIM_SUFFIX, FIM_PREFIX, FIM_MIDDLE, FIM_PAD]\n",
        "        )\n",
        "    except KeyError:\n",
        "        suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id = None, None, None, None\n",
        "    return suffix_tok_id, prefix_tok_id, middle_tok_id, pad_tok_id\n",
        "\n",
        "\n",
        "## Adapted from https://github.com/bigcode-project/Megatron-LM/blob/6c4bf908df8fd86b4977f54bf5b8bd4b521003d1/megatron/data/gpt_dataset.py\n",
        "def permute(\n",
        "    sample,\n",
        "    np_rng,\n",
        "    suffix_tok_id,\n",
        "    prefix_tok_id,\n",
        "    middle_tok_id,\n",
        "    pad_tok_id,\n",
        "    fim_rate=0.5,\n",
        "    fim_spm_rate=0.5,\n",
        "    truncate_or_pad=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Take in a sample (list of tokens) and perform a FIM transformation on it with a probability of fim_rate, using two FIM modes:\n",
        "    PSM and SPM (with a probability of fim_spm_rate).\n",
        "    \"\"\"\n",
        "\n",
        "    # The if condition will trigger with the probability of fim_rate\n",
        "    # This means FIM transformations will apply to samples with a probability of fim_rate\n",
        "    if np_rng.binomial(1, fim_rate):\n",
        "\n",
        "        # Split the sample into prefix, middle, and suffix, based on randomly generated indices stored in the boundaries list.\n",
        "        boundaries = list(np_rng.randint(low=0, high=len(sample) + 1, size=2))\n",
        "        boundaries.sort()\n",
        "\n",
        "        prefix = np.array(sample[: boundaries[0]], dtype=np.int64)\n",
        "        middle = np.array(sample[boundaries[0] : boundaries[1]], dtype=np.int64)\n",
        "        suffix = np.array(sample[boundaries[1] :], dtype=np.int64)\n",
        "\n",
        "        if truncate_or_pad:\n",
        "            # calculate the new total length of the sample, taking into account tokens indicating prefix, middle, and suffix\n",
        "            new_length = suffix.shape[0] + prefix.shape[0] + middle.shape[0] + 3\n",
        "            diff = new_length - len(sample)\n",
        "\n",
        "            # trancate or pad if there's a difference in length between the new length and the original\n",
        "            if diff > 0:\n",
        "                if suffix.shape[0] <= diff:\n",
        "                    return sample, np_rng\n",
        "                suffix = suffix[: suffix.shape[0] - diff]\n",
        "            elif diff < 0:\n",
        "                suffix = np.concatenate([suffix, np.full((-1 * diff), pad_tok_id)])\n",
        "\n",
        "        # With the probability of fim_spm_rateapply SPM variant of FIM transformations\n",
        "        # SPM: suffix, prefix, middle\n",
        "        if np_rng.binomial(1, fim_spm_rate):\n",
        "            new_sample = np.concatenate(\n",
        "                [\n",
        "                    [prefix_tok_id, suffix_tok_id],\n",
        "                    suffix,\n",
        "                    [middle_tok_id],\n",
        "                    prefix,\n",
        "                    middle,\n",
        "                ]\n",
        "            )\n",
        "        # Otherwise, apply the PSM variant of FIM transformations\n",
        "        # PSM: prefix, suffix, middle\n",
        "        else:\n",
        "\n",
        "            new_sample = np.concatenate(\n",
        "                [\n",
        "                    [prefix_tok_id],\n",
        "                    prefix,\n",
        "                    [suffix_tok_id],\n",
        "                    suffix,\n",
        "                    [middle_tok_id],\n",
        "                    middle,\n",
        "                ]\n",
        "            )\n",
        "    else:\n",
        "        # don't apply FIM transformations\n",
        "        new_sample = sample\n",
        "\n",
        "    return list(new_sample), np_rng\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwW5FviD9xBH"
      },
      "source": [
        "让我们定义 `ConstantLengthDataset`，这是一个可迭代的数据集，它将返回固定长度的 token 块。为此，我们将从原始数据集中读取文本缓冲区，直到达到大小限制，然后应用分词器将原始文本转换为 token 后的输入。可选项，我们可以在一些序列上执行 FIM 变换（受影响的序列比例由 `fim_rate` 控制）。\n",
        "\n",
        "定义好后，我们可以从训练和验证数据中创建 `ConstantLengthDataset` 的实例。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AgDW-692wzOl",
        "outputId": "bf8dd724-e3f0-4ffe-bee2-d5495a953ed4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIM is not supported by tokenizer, disabling FIM\n",
            "FIM is not supported by tokenizer, disabling FIM\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import random\n",
        "\n",
        "# Create an Iterable dataset that returns constant-length chunks of tokens from a stream of text files.\n",
        "\n",
        "class ConstantLengthDataset(IterableDataset):\n",
        "    \"\"\"\n",
        "    Iterable dataset that returns constant length chunks of tokens from stream of text files.\n",
        "        Args:\n",
        "            tokenizer (Tokenizer): The processor used for proccessing the data.\n",
        "            dataset (dataset.Dataset): Dataset with text files.\n",
        "            infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n",
        "            seq_length (int): Length of token sequences to return.\n",
        "            num_of_sequences (int): Number of token sequences to keep in buffer.\n",
        "            chars_per_token (int): Number of characters per token used to estimate number of tokens in text buffer.\n",
        "            fim_rate (float): Rate (0.0 to 1.0) that sample will be permuted with FIM.\n",
        "            fim_spm_rate (float): Rate (0.0 to 1.0) of FIM permuations that will use SPM.\n",
        "            seed (int): Seed for random number generator.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        dataset,\n",
        "        infinite=False,\n",
        "        seq_length=1024,\n",
        "        num_of_sequences=1024,\n",
        "        chars_per_token=3.6,\n",
        "        content_field=\"content\",\n",
        "        fim_rate=0.5,\n",
        "        fim_spm_rate=0.5,\n",
        "        seed=0,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.concat_token_id = tokenizer.eos_token_id\n",
        "        self.dataset = dataset\n",
        "        self.seq_length = seq_length\n",
        "        self.infinite = infinite\n",
        "        self.current_size = 0\n",
        "        self.max_buffer_size = seq_length * chars_per_token * num_of_sequences\n",
        "        self.content_field = content_field\n",
        "        self.fim_rate = fim_rate\n",
        "        self.fim_spm_rate = fim_spm_rate\n",
        "        self.seed = seed\n",
        "\n",
        "        (\n",
        "            self.suffix_tok_id,\n",
        "            self.prefix_tok_id,\n",
        "            self.middle_tok_id,\n",
        "            self.pad_tok_id,\n",
        "        ) = get_fim_token_ids(self.tokenizer)\n",
        "        if not self.suffix_tok_id and self.fim_rate > 0:\n",
        "            print(\"FIM is not supported by tokenizer, disabling FIM\")\n",
        "            self.fim_rate = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        iterator = iter(self.dataset)\n",
        "        more_examples = True\n",
        "        np_rng = np.random.RandomState(seed=self.seed)\n",
        "        while more_examples:\n",
        "            buffer, buffer_len = [], 0\n",
        "            while True:\n",
        "                if buffer_len >= self.max_buffer_size:\n",
        "                    break\n",
        "                try:\n",
        "                    buffer.append(next(iterator)[self.content_field])\n",
        "                    buffer_len += len(buffer[-1])\n",
        "                except StopIteration:\n",
        "                    if self.infinite:\n",
        "                        iterator = iter(self.dataset)\n",
        "                    else:\n",
        "                        more_examples = False\n",
        "                        break\n",
        "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
        "            all_token_ids = []\n",
        "\n",
        "            for tokenized_input in tokenized_inputs:\n",
        "                # optionally do FIM permutations\n",
        "                if self.fim_rate > 0:\n",
        "                    tokenized_input, np_rng = permute(\n",
        "                        tokenized_input,\n",
        "                        np_rng,\n",
        "                        self.suffix_tok_id,\n",
        "                        self.prefix_tok_id,\n",
        "                        self.middle_tok_id,\n",
        "                        self.pad_tok_id,\n",
        "                        fim_rate=self.fim_rate,\n",
        "                        fim_spm_rate=self.fim_spm_rate,\n",
        "                        truncate_or_pad=False,\n",
        "                    )\n",
        "\n",
        "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
        "            examples = []\n",
        "            for i in range(0, len(all_token_ids), self.seq_length):\n",
        "                input_ids = all_token_ids[i : i + self.seq_length]\n",
        "                if len(input_ids) == self.seq_length:\n",
        "                    examples.append(input_ids)\n",
        "            random.shuffle(examples)\n",
        "            for example in examples:\n",
        "                self.current_size += 1\n",
        "                yield {\n",
        "                    \"input_ids\": torch.LongTensor(example),\n",
        "                    \"labels\": torch.LongTensor(example),\n",
        "                }\n",
        "\n",
        "\n",
        "train_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        train_data,\n",
        "        infinite=True,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        seed=SEED,\n",
        ")\n",
        "eval_dataset = ConstantLengthDataset(\n",
        "        tokenizer,\n",
        "        valid_data,\n",
        "        infinite=False,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        chars_per_token=chars_per_token,\n",
        "        content_field=DATA_COLUMN,\n",
        "        fim_rate=FIM_RATE,\n",
        "        fim_spm_rate=FIM_SPM_RATE,\n",
        "        seed=SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U bitsandbytes\n",
        "!pip index versions bitsandbytes"
      ],
      "metadata": {
        "id": "OZOGULAy3i7f",
        "outputId": "c15491b7-59d6-4480-dc98-3b88f7ab8a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pip index is currently an experimental command. It may be removed/changed in a future release without prior warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mbitsandbytes (0.45.0)\n",
            "Available versions: 0.45.0, 0.44.1, 0.44.0, 0.43.3, 0.43.2, 0.43.1, 0.43.0, 0.42.0, 0.41.3.post2, 0.41.3.post1, 0.41.3, 0.41.2.post2, 0.41.2.post1, 0.41.2, 0.41.1, 0.41.0, 0.40.2, 0.40.1.post1, 0.40.1, 0.40.0.post4, 0.40.0.post3, 0.40.0.post2, 0.40.0.post1, 0.40.0, 0.39.1, 0.39.0, 0.38.1, 0.38.0.post2, 0.38.0.post1, 0.38.0, 0.37.2, 0.37.1, 0.37.0, 0.36.0.post2, 0.36.0.post1, 0.36.0, 0.35.4, 0.35.3, 0.35.2, 0.35.1, 0.35.0, 0.34.0, 0.33.1, 0.33.0, 0.32.3, 0.32.2, 0.32.1, 0.32.0, 0.31.8\n",
            "  INSTALLED: 0.45.0\n",
            "  LATEST:    0.45.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxev1sk6tRW9"
      },
      "source": [
        "## 准备模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCtWV-U42Eq_"
      },
      "source": [
        "现在数据已经准备好了，是时候加载模型了！我们将加载量化的模型。\n",
        "\n",
        "因为量化使用更少的位来表示数据，所以会减少内存使用。我们将使用 `bitsandbytes` 库来量化模型，因为它与 `transformers` 有很好的集成。我们需要做的只是定义一个 `bitsandbytes` 配置，然后在加载模型时使用它。\n",
        "\n",
        "4 比特位量化有不同的变体，但通常我们推荐使用 NF4 量化以获得更好的性能（`bnb_4bit_quant_type=\"nf4\"`）。\n",
        "\n",
        "`bnb_4bit_use_double_quant` 选项在第一次量化后添加第二次量化，以节省每个参数额外的 0.4 位。\n",
        "\n",
        "要了解更多关于量化的信息，请查看 [\"利用 bitsandbytes、4 比特位量化和 QLoRA 让 LLMs 更易于访问\" 的博客](https://huggingface.co/blog/4bit-transformers-bitsandbytes)。\n",
        "\n",
        "定义好后，将配置传递给 `from_pretrained` 方法以加载量化的模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XuwoX6U2DUvK",
        "outputId": "42ebec26-299d-4ee8-8d71-88edb32bb698",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-24a1859796bb>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_in_8bit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3657\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3658\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3659\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft.tuners.lora import LoraLayer\n",
        "import bitsandbytes\n",
        "\n",
        "load_in_8bit = False\n",
        "\n",
        "# 4-bit quantization\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=device_map,\n",
        "        use_cache=False,  # We will be using gradient checkpointing\n",
        "        trust_remote_code=True,\n",
        "        use_flash_attention_2=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO9e2FV8D8ZF"
      },
      "source": [
        "当使用量化模型进行训练时，你需要调用 `prepare_model_for_kbit_training()` 函数来预处理量化模型以进行训练。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb_eB4xzEDBk"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmnLjPZpDVtg"
      },
      "source": [
        "现在量化模型已经准备好了，我们可以设置一个 LoRA 配置。LoRA 通过大幅减少可训练参数的数量，使得微调更加高效。\n",
        "\n",
        "要使用 LoRA 技术训练模型，我们需要将基础模型包装为 `PeftModel`。这涉及到使用 `LoraConfig` 定义 LoRA 配置，并使用 `get_peft_model()` 和 `LoraConfig` 包装原始模型。\n",
        "\n",
        "要了解更多关于 LoRA 及其参数的信息，请参考 [PEFT 文档](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pAUU2FR2Gey",
        "outputId": "63328c2b-e693-49b1-ce0a-3ca8722f852a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 5,554,176 || all params: 1,142,761,472 || trainable%: 0.4860310866343243\n"
          ]
        }
      ],
      "source": [
        "# Set up lora\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=LORA_TARGET_MODULES.split(\",\"),\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHe7AElXzXVV"
      },
      "source": [
        "可以看到，通过应用 LoRA 技术，我们现在只需要训练不到 1% 的参数。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_CqVydc40IM"
      },
      "source": [
        "## 训练模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_iN2khjrbD3"
      },
      "source": [
        "现在我们已经准备好了数据，并且优化了模型，我们可以将所有东西整合在一起开始训练。\n",
        "\n",
        "要实例化一个 `Trainer`，你需要定义训练配置。最重要的是 `TrainingArguments`，这是一个包含所有用于配置训练的属性的类。\n",
        "\n",
        "这些与你可能运行的任何其他类型的模型训练相似，所以我们这里不会详细说明。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65QHS8l1tKQe"
      },
      "outputs": [],
      "source": [
        "train_data.start_iteration = 0\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"Your_HF_username/{OUTPUT_DIR}\",\n",
        "    dataloader_drop_last=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    max_steps=MAX_STEPS,\n",
        "    eval_steps=EVAL_FREQ,\n",
        "    save_steps=SAVE_FREQ,\n",
        "    logging_steps=LOG_FREQ,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    warmup_steps=NUM_WARMUP_STEPS,\n",
        "    gradient_accumulation_steps=GR_ACC_STEPS,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=FP16,\n",
        "    bf16=BF16,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    push_to_hub=True,\n",
        "    include_tokens_per_second=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB_fLRex09ut"
      },
      "source": [
        "最后一步，实例化 `Trainer` 并调用 `train` 方法。   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rS3nVwhUC69O",
        "outputId": "61a5bdb2-b7d0-4aed-8290-4bf20c2ccd38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 4:16:10, Epoch 1/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.524600</td>\n",
              "      <td>7.456872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>5.617800</td>\n",
              "      <td>7.262190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>5.129100</td>\n",
              "      <td>6.410039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>5.052200</td>\n",
              "      <td>6.306774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.202900</td>\n",
              "      <td>6.117062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>4.654100</td>\n",
              "      <td>6.018349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>5.100200</td>\n",
              "      <td>6.000355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>5.049800</td>\n",
              "      <td>5.889457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>4.541200</td>\n",
              "      <td>5.813823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.000700</td>\n",
              "      <td>5.834208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>5.026500</td>\n",
              "      <td>5.781939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>4.411800</td>\n",
              "      <td>5.720596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>4.782500</td>\n",
              "      <td>5.736376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>4.980200</td>\n",
              "      <td>5.712276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>4.368700</td>\n",
              "      <td>5.689637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>4.884700</td>\n",
              "      <td>5.675920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>4.914400</td>\n",
              "      <td>5.662421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>4.248700</td>\n",
              "      <td>5.660122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>4.798400</td>\n",
              "      <td>5.664026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>4.704200</td>\n",
              "      <td>5.655665</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=4.885598585128784, metrics={'train_runtime': 15380.3075, 'train_samples_per_second': 2.081, 'train_steps_per_second': 0.13, 'train_tokens_per_second': 4261.033, 'total_flos': 4.0317260660736e+17, 'train_loss': 4.885598585128784, 'epoch': 1.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "print(\"Training...\")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAERlCnt1PEW"
      },
      "source": [
        "最后，你可以将微调好的模型推送到你的 Hub 仓库中，并分享给你的团队。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h7_AUTTDwE1"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBVH7uFOM_UF"
      },
      "source": [
        "## 推理\n",
        "\n",
        "一旦模型被上传到 Hub，我们就可以使用它进行推理。为此，我们首先初始化原始的基础模型及其分词器。接下来，我们需要将微调后的权重与基础模型合并。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtL37piINBFe"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# load the original model first\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    quantization_config=None,\n",
        "    device_map=None,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ").cuda()\n",
        "\n",
        "# merge fine-tuned weights with the base model\n",
        "peft_model_id = f\"Your_HF_username/{OUTPUT_DIR}\"\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
        "model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3USQ2suvDi9M"
      },
      "source": [
        "现在我们可以使用合并后的模型进行推理。为了方便起见，我们将定义一个 `get_code_completion` 函数 - 请随意尝试文本生成参数！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoTGpNbjDeWI"
      },
      "outputs": [],
      "source": [
        "def get_code_completion(prefix, suffix):\n",
        "    text = prompt = f\"\"\"{prefix}{suffix}\"\"\"\n",
        "    model.eval()\n",
        "    outputs = model.generate(\n",
        "        input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.cuda(),\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.2,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.0,\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kMJiGDfDrBf"
      },
      "source": [
        "现在，为了获得代码补全，我们只需要调用 `get_code_complete` 函数，并将我们希望补全的前几行作为前缀传递，以及一个空字符串作为后缀。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXlco2_-YcvM",
        "outputId": "41c411ad-b7dc-4277-f975-c173888234bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from peft import LoraConfig, TaskType, get_peft_model\n",
            "from transformers import AutoModelForCausalLM\n",
            "peft_config = LoraConfig(\n",
            "    task_type=TaskType.CAUSAL_LM,\n",
            "    r=8,\n",
            "    lora_alpha=32,\n",
            "    target_modules=[\"q_proj\", \"v_proj\"],\n",
            "    lora_dropout=0.1,\n",
            "    bias=\"none\",\n",
            "    modules_to_save=[\"q_proj\", \"v_proj\"],\n",
            "    inference_mode=False,\n",
            ")\n",
            "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "model = get_peft_model(model, peft_config)\n",
            "model.print_trainable_parameters()\n"
          ]
        }
      ],
      "source": [
        "prefix = \"\"\"from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import AutoModelForCausalLM\n",
        "peft_config = LoraConfig(\n",
        "\"\"\"\n",
        "suffix =\"\"\"\"\"\"\n",
        "\n",
        "print(get_code_completion(prefix, suffix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql2563kGlnmu"
      },
      "source": [
        "作为刚刚在这个 notebook 中使用过 PEFT 库的人，你可以看到创建为 `LoraConfig` 函数的生成结果相当不错！\n",
        "\n",
        "如果你回到我们为推理实例化模型的单元格，并注释掉我们合并微调权重的行，你可以看到原始模型对于完全相同的前缀会生成什么内容："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29xxp1eHTgJ9",
        "outputId": "c6d597a2-01da-4d25-a32f-3a551212c5b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from peft import LoraConfig, TaskType, get_peft_model\n",
            "from transformers import AutoModelForCausalLM\n",
            "peft_config = LoraConfig(\n",
            "    model_name_or_path=\"facebook/wav2vec2-base-960h\",\n",
            "    num_labels=1,\n",
            "    num_features=1,\n",
            "    num_hidden_layers=1,\n",
            "    num_attention_heads=1,\n",
            "    num_hidden_layers_per_attention_head=1,\n",
            "    num_attention_heads_per_hidden_layer=1,\n",
            "    hidden_size=1024,\n",
            "    hidden_dropout_prob=0.1,\n",
            "    hidden_act=\"gelu\",\n",
            "    hidden_act_dropout_prob=0.1,\n",
            "    hidden\n"
          ]
        }
      ],
      "source": [
        "prefix = \"\"\"from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import AutoModelForCausalLM\n",
        "peft_config = LoraConfig(\n",
        "\"\"\"\n",
        "suffix =\"\"\"\"\"\"\n",
        "\n",
        "print(get_code_completion(prefix, suffix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwy2ZC7U8Ema"
      },
      "source": [
        "尽管这是 Python 语法，但你可以看到原始模型并不理解 `LoraConfig` 应该做什么。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CATYE8pp2drQ"
      },
      "source": [
        "要了解这种高效参数微调与完全微调的比较，以及如何通过推理端点在 VS Code 中使用这样的模型作为你的编程助手(copilot)，或者在本地使用，请查看[\"个人编程助手(copilot)：训练你自己的编码助手\"博客](https://huggingface.co/blog/personal-copilot)。这个 notebook 补充了原始博客内容。\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "499f308a33e342b3869f35cf9ac5eb67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_939c40c9931646a8b609d81b678511d1"
          }
        },
        "4cef64cd3d5b4fe699008fc40ec612c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cc426f9bf61475e99f8056a482a4c80",
            "placeholder": "​",
            "style": "IPY_MODEL_46198864372549df8a442e81257e3837",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a1c6a15eaec34748ab1e75ec1add3b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3f52234077e242a19600f823412232fb",
            "placeholder": "​",
            "style": "IPY_MODEL_320b27911e994b6590f5dd8e040463cc",
            "value": ""
          }
        },
        "dc87041381b94558939f626b2a93e94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_16a663c77fd74a6e8f89af60ade0c8d4",
            "style": "IPY_MODEL_2e20951121bd42e0b6589a3bafc48ef5",
            "value": true
          }
        },
        "b167e021a19e420fa91f876f81a7ab12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c212a6553c4a40908417006181452847",
            "style": "IPY_MODEL_6c09de25b5f84813a189bee2ef2e1a36",
            "tooltip": ""
          }
        },
        "ca8e7935581a4d67ad0e9b7f4f2ed128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b390ada147664c8192dc6ff802961129",
            "placeholder": "​",
            "style": "IPY_MODEL_758dd23fb8fa4dfa9aaf5ad754dc6ec4",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "939c40c9931646a8b609d81b678511d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6cc426f9bf61475e99f8056a482a4c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46198864372549df8a442e81257e3837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f52234077e242a19600f823412232fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320b27911e994b6590f5dd8e040463cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16a663c77fd74a6e8f89af60ade0c8d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e20951121bd42e0b6589a3bafc48ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c212a6553c4a40908417006181452847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c09de25b5f84813a189bee2ef2e1a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b390ada147664c8192dc6ff802961129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "758dd23fb8fa4dfa9aaf5ad754dc6ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9e97434bb8541fa9a85b6248b3350c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d003c198c8794e7f9138f4382189c62b",
            "placeholder": "​",
            "style": "IPY_MODEL_7aecccd20ea24efdb3778e9063b9cfe6",
            "value": "Connecting..."
          }
        },
        "d003c198c8794e7f9138f4382189c62b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aecccd20ea24efdb3778e9063b9cfe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}